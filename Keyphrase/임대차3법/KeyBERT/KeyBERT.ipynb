{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91a11FXEVewl",
        "outputId": "afab8ffa-e0f5-416a-9142-58af921c7fd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVaMHuWDU0Bm"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "apt-get update\n",
        "apt-get install g++ openjdk-8-jdk python-dev python3-dev\n",
        "pip3 install JPype1\n",
        "pip3 install konlpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISM31f0XU0zj",
        "outputId": "1802cf5e-fb88-422e-b968-c4affcf5030f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: JAVA_HOME=\"/usr/lib/jvm/java-8-openjdk-amd64\"\n"
          ]
        }
      ],
      "source": [
        "%env JAVA_HOME \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_l3-4YVVDIq",
        "outputId": "ad19caf4-cd55-4af2-db39-7e3ea0ec7a5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 3.1 MB/s \n",
            "\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.22.2-py3-none-any.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 24.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.64.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.13.1+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.7.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.7)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 58.1 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
            "  Downloading huggingface_hub-0.10.0-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 65.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.1.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (5.0.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 53.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.6.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers) (3.8.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=4b7ad1b8ae361baaaa1303df4281ba0772d0f17537db411f3a6efc558bac3a5d\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/06/fb/d59c1e5bd1dac7f6cf61ec0036cc3a10ab8fecaa6b2c3d3ee9\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers, sentencepiece, sentence-transformers\n",
            "Successfully installed huggingface-hub-0.10.0 sentence-transformers-2.2.2 sentencepiece-0.1.97 tokenizers-0.12.1 transformers-4.22.2\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TVnQwX9eUlLw"
      },
      "outputs": [],
      "source": [
        "from glob import glob\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Gkiu17hHUlLy"
      },
      "outputs": [],
      "source": [
        "def load_data(file, sheet_name):\n",
        "    return pd.read_excel(file, sheet_name=sheet_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "TIoB6W-iUlLy"
      },
      "outputs": [],
      "source": [
        "def load_all_data():\n",
        "    files = glob('/content/drive/MyDrive/aida/dataset/임대차3법(54,752건)/*.xlsx')\n",
        "    df = pd.DataFrame()\n",
        "    for file in files:\n",
        "        df = df.append(load_data(file, '뉴스'))\n",
        "    return df\n",
        "\n",
        "news = load_all_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6B95YRlUlLy",
        "outputId": "ed89a9c7-5bbf-47a3-894b-e74d11f255d5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(17514, 4)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "news.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "MUG6P-NYUlLz"
      },
      "outputs": [],
      "source": [
        "def text_cleaning(x):\n",
        "    mail_del = re.sub(\"([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z-.]+)\", \"\", str(x))\n",
        "    meta_del = re.sub(\"[\\r\\n\\xa0]\", \"\", str(mail_del))\n",
        "    name_del = re.sub(\"(\\.\\s+[ㄱ-ㅎ가-힣]+\\s[기]+[자]+)\", \"\", str(meta_del))\n",
        "    clean_text = re.sub(\"[^\\w\\s^.]\", \" \", name_del)\n",
        "    \n",
        "    return clean_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "jY-jZzbeUlLz",
        "outputId": "327bba05-d28d-4c6c-db0f-8722aed16e12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(17497, 4)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-7508e6d8-624d-4635-b564-dab9894a849d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>작성일</th>\n",
              "      <th>작성자</th>\n",
              "      <th>제목</th>\n",
              "      <th>내용</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020/11/17</td>\n",
              "      <td>MBC연예</td>\n",
              "      <td>'100분토론' 서민 주거 안정 위한 '임대차 3법', 10명 중 6명은 모른다?</td>\n",
              "      <td>오늘 17일  방송되는 MBC  100분 토론 에서는  전세난 해법은  이라는 주제...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020/08/12</td>\n",
              "      <td>오마이뉴스</td>\n",
              "      <td>임대차3법 탓에 10억→14억?... 언론은 어떻게 왜곡하나</td>\n",
              "      <td>보도 검증  기사에 거론된 아파트들 찾아가보니... 원래 14억대 거래... 10...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020/07/29</td>\n",
              "      <td>연합뉴스</td>\n",
              "      <td>\"임대차 3법, 주거안정 기여 기대\"…매물잠김 우려도(종합)</td>\n",
              "      <td>전문가들  긍정 평가속 시행 초기 부작용 우려 의사봉 두드리는 윤호중 법사위원장  ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020/07/29</td>\n",
              "      <td>연합뉴스</td>\n",
              "      <td>'전세난민' 사라지나…전문가 \"임대차3법, 주거안정에 도움\"</td>\n",
              "      <td>전세매물 잠김현상 4년 주기 전셋값 폭등 우려도 의사봉 두드리는 윤호중 법사위원장 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2020/11/16</td>\n",
              "      <td>한겨레</td>\n",
              "      <td>[안재승 칼럼] ‘동네북’ 임대차 3법, 더 강력해져야 한다</td>\n",
              "      <td>임대차 3법이 동네북 신세다. 보수 야당과 언론이 임대차 3법이 전세난을 불렀다고 ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7508e6d8-624d-4635-b564-dab9894a849d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7508e6d8-624d-4635-b564-dab9894a849d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7508e6d8-624d-4635-b564-dab9894a849d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "0         작성일    작성자                                             제목  \\\n",
              "1  2020/11/17  MBC연예  '100분토론' 서민 주거 안정 위한 '임대차 3법', 10명 중 6명은 모른다?   \n",
              "2  2020/08/12  오마이뉴스              임대차3법 탓에 10억→14억?... 언론은 어떻게 왜곡하나   \n",
              "3  2020/07/29   연합뉴스              \"임대차 3법, 주거안정 기여 기대\"…매물잠김 우려도(종합)   \n",
              "4  2020/07/29   연합뉴스              '전세난민' 사라지나…전문가 \"임대차3법, 주거안정에 도움\"   \n",
              "5  2020/11/16    한겨레              [안재승 칼럼] ‘동네북’ 임대차 3법, 더 강력해져야 한다   \n",
              "\n",
              "0                                                 내용  \n",
              "1  오늘 17일  방송되는 MBC  100분 토론 에서는  전세난 해법은  이라는 주제...  \n",
              "2   보도 검증  기사에 거론된 아파트들 찾아가보니... 원래 14억대 거래... 10...  \n",
              "3  전문가들  긍정 평가속 시행 초기 부작용 우려 의사봉 두드리는 윤호중 법사위원장  ...  \n",
              "4  전세매물 잠김현상 4년 주기 전셋값 폭등 우려도 의사봉 두드리는 윤호중 법사위원장 ...  \n",
              "5  임대차 3법이 동네북 신세다. 보수 야당과 언론이 임대차 3법이 전세난을 불렀다고 ...  "
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "news.columns = news.iloc[0]\n",
        "news.drop(0, inplace=True, axis=0)\n",
        "news = news.dropna(axis=0, how='any')\n",
        "\n",
        "news['내용'] = news['내용'].map(text_cleaning)\n",
        "\n",
        "print(news.shape)\n",
        "news.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "M4nacjS3UlLz"
      },
      "outputs": [],
      "source": [
        "def max_sum_sim(doc_embedding, candidate_embeddings, words, top_n, nr_candidates):\n",
        "    distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
        "    distances_candidates = cosine_similarity(candidate_embeddings, \n",
        "                                            candidate_embeddings)\n",
        "    words_idx = list(distances.argsort()[0][-nr_candidates:])\n",
        "    words_vals = [candidates[index] for index in words_idx]\n",
        "    distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\n",
        "\n",
        "    min_sim = np.inf\n",
        "    candidate = None\n",
        "    for combination in itertools.combinations(range(len(words_idx)), top_n):\n",
        "        sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])\n",
        "        if sim < min_sim:\n",
        "            candidate = combination\n",
        "            min_sim = sim\n",
        "\n",
        "    return [words_vals[idx] for idx in candidate]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "4MyEK0fvUlL0"
      },
      "outputs": [],
      "source": [
        "# 이미 fine-tuning되어있는 open-source model이 있어 그걸 사용, cost 낭비X\n",
        "class mySBERT(SentenceTransformer):\n",
        "    def __init__(self, path, modules=None):\n",
        "        super().__init__(path, modules)\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.tokenizer = self._first_module().tokenizer\n",
        "\n",
        "    def encode(self, sentences, batch_size=32, show_progress_bar=None, output_value='token_embeddings', convert_to_numpy=True, convert_to_tensor=False, is_pretokenized=False):\n",
        "        return super().encode(sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, is_pretokenized)\n",
        "\n",
        "    def tokenize(self, sentences):\n",
        "        return self.tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    \n",
        "    def to(self, device):\n",
        "        self.device = device\n",
        "        return self"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKE4e91gUlL0",
        "outputId": "abd30b14-3f10-4700-af68-02af29fd8936"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nr_candidates=10 : \n",
            "0th instance\n",
            "['임대차 전월세', '이후 전월세', '분석 전월세', '비율 전월세', '토론 전월세']\n",
            "---------------------------------------------------\n",
            "1th instance\n",
            "['임대차 때문', '시세 계약', '전월세 시행', '전세 시세', '임대차 부작용']\n",
            "---------------------------------------------------\n",
            "2th instance\n",
            "['주택임대차보호법 개정', '시행 월세', '전망 전월세', '지적 주택임대차보호법', '임대차 개정안']\n",
            "---------------------------------------------------\n",
            "3th instance\n",
            "['고려 전월세', '전세 매물', '임대차 개정안', '부담 전세', '통과 주택임대차보호법']\n",
            "---------------------------------------------------\n",
            "4th instance\n",
            "['임대차 동네', '철렁 집주인', '청구권 전월세', '집주인 반발', '집주인 권리']\n",
            "---------------------------------------------------\n",
            "5th instance\n",
            "['상률 주택임대차보호법', '임대료 인상', '임대료 폭등', '주장 주택임대차보호법', '권제 전월세']\n",
            "---------------------------------------------------\n",
            "6th instance\n",
            "['청구권 전월세', '전월세 고제', '제한 집주인', '전셋집 워낙', '집주인 입자']\n",
            "---------------------------------------------------\n",
            "7th instance\n",
            "['임대차 셋값', '전월세 거래', '아파트 셋값', '전셋집 자금', '전세 보증금']\n",
            "---------------------------------------------------\n",
            "8th instance\n",
            "['임대차 소급', '세법 임대차', '청구권 전월세', '전월세 시장', '전월세 핵심']\n",
            "---------------------------------------------------\n",
            "9th instance\n",
            "['전월세 고제', '임대차 전월세', '전월세 계약', '응답 전월세', '영향 전월세']\n",
            "---------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "okt = Okt()\n",
        "model = SentenceTransformer('jhgan/ko-sbert-multitask')\n",
        "print('nr_candidates=10 : ')\n",
        "\n",
        "for i in range(10):\n",
        "    tokenized_doc = okt.pos(news.iloc[i]['내용'], norm=True, stem=True)\n",
        "    tokenized_nouns = ' '.join([word[0] for word in tokenized_doc if word[1] == 'Noun'])\n",
        "\n",
        "    n_gram_range = (1,2)\n",
        "\n",
        "    count = CountVectorizer(ngram_range=n_gram_range).fit([tokenized_nouns])\n",
        "    candidates = count.get_feature_names_out()\n",
        "\n",
        "    candidate_embeddings = model.encode(candidates, convert_to_tensor=True)\n",
        "    doc_embedding = model.encode([tokenized_nouns], convert_to_tensor=True)\n",
        "    \n",
        "    keywords = max_sum_sim(doc_embedding.cpu(), candidate_embeddings.cpu(), candidates, top_n=5, nr_candidates=10)\n",
        "    \n",
        "    print(f'{i}th instance')\n",
        "    print(keywords)\n",
        "    print('---------------------------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sX6hd_EUlL0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.13 ('ML')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "19d29624fa02f72a2f2eb64b5fa4dfbc751609e2b6c88be691c0db207c64cc14"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
