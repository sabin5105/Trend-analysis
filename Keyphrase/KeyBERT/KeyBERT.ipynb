{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "apt-get update\n",
        "apt-get install g++ openjdk-8-jdk python-dev python3-dev\n",
        "pip3 install JPype1\n",
        "pip3 install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gWfXF-tbjh9",
        "outputId": "66572d87-b1f8-49e2-c5cd-b4a1c946b534"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Hit:2 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:7 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:9 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [83.3 kB]\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Fetched 261 kB in 2s (135 kB/s)\n",
            "Reading package lists...\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "python-dev is already the newest version (2.7.15~rc1-1).\n",
            "g++ is already the newest version (4:7.4.0-1ubuntu2.3).\n",
            "python3-dev is already the newest version (3.6.7-1~18.04).\n",
            "openjdk-8-jdk is already the newest version (8u342-b07-0ubuntu1~18.04).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: JPype1 in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from JPype1) (21.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->JPype1) (3.0.9)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.7/dist-packages (0.6.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.9.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (21.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->JPype1>=0.7.0->konlpy) (3.0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%env JAVA_HOME \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOHZ0BBdbq-G",
        "outputId": "8a9400d9-67bc-49c2-a507-0f2bc7fff74f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: JAVA_HOME=\"/usr/lib/jvm/java-8-openjdk-amd64\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMssDox-bsUx",
        "outputId": "bfa22c47-2220-4243-9148-3b688b13986a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.2.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.97)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.7.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.7)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.64.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.13.1+cu113)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.6)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.12.1+cu113)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.23.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.8.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.13.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers) (3.9.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.9.24)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SynTZ-UUi-2b",
        "outputId": "6ff09bb0-e69f-4001-e473-635816005592"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YTUE27pabh5S"
      },
      "outputs": [],
      "source": [
        "from glob import glob\n",
        "import re\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_8EfI5Rzbh5W"
      },
      "outputs": [],
      "source": [
        "def load_data(file, sheet_name):\n",
        "    return pd.read_excel(file, sheet_name=sheet_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TESDofYUbh5X"
      },
      "outputs": [],
      "source": [
        "def load_all_data(path):\n",
        "    files = glob(path+'/*.xlsx')\n",
        "    df = pd.DataFrame()\n",
        "    for file in files:\n",
        "        df = df.append(load_data(file, '뉴스'))\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "o_tu4zyPbh5a"
      },
      "outputs": [],
      "source": [
        "def text_cleaning(x):\n",
        "    mail_del = re.sub(\"([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z-.]+)\", \"\", str(x))\n",
        "    meta_del = re.sub(\"[\\r\\n\\xa0]\", \"\", str(mail_del))\n",
        "    name_del = re.sub(\"(\\.\\s+[ㄱ-ㅎ가-힣]+\\s[기]+[자]+)\", \"\", str(meta_del))\n",
        "    clean_text = re.sub(\"[^\\w\\s^.]\", \" \", name_del)\n",
        "    \n",
        "    return clean_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "pBDTXh6zbh5b"
      },
      "outputs": [],
      "source": [
        "def max_sum_sim(doc_embedding, candidate_embeddings, words, top_n, nr_candidates):\n",
        "    distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
        "    distances_candidates = cosine_similarity(candidate_embeddings, \n",
        "                                            candidate_embeddings)\n",
        "    words_idx = list(distances.argsort()[0][-nr_candidates:])\n",
        "    words_vals = [candidates[index] for index in words_idx]\n",
        "    distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\n",
        "\n",
        "    min_sim = np.inf\n",
        "    candidate = None\n",
        "    for combination in itertools.combinations(range(len(words_idx)), top_n):\n",
        "        sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])\n",
        "        if sim < min_sim:\n",
        "            candidate = combination\n",
        "            min_sim = sim\n",
        "\n",
        "    return [words_vals[idx] for idx in candidate]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "oneWBmiAbh5c"
      },
      "outputs": [],
      "source": [
        "class mySBERT(SentenceTransformer):\n",
        "    def __init__(self, path, modules=None):\n",
        "        super().__init__(path, modules)\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.tokenizer = self._first_module().tokenizer\n",
        "\n",
        "    def encode(self, sentences, batch_size=32, show_progress_bar=None, output_value='token_embeddings', convert_to_numpy=True, convert_to_tensor=False, is_pretokenized=False):\n",
        "        return super().encode(sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, is_pretokenized)\n",
        "\n",
        "    def tokenize(self, sentences):\n",
        "        return self.tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    \n",
        "    def to(self, device):\n",
        "        self.device = device\n",
        "        return self"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "gb_nBFxCbh5d",
        "outputId": "a5256e99-4698-47a7-b409-3b524af69e5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [57:40<00:00,  2.89it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-d1347e696dea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/aida/neutral.json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ascii\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
          ]
        }
      ],
      "source": [
        "for i in range(1,5):\n",
        "    path = \"/content/drive/MyDrive/aida/dataset\"\n",
        "    save_path = \"/content/drive/MyDrive/aida\"\n",
        "    if i==1:\n",
        "        path += \"/임대차3법(54,752건)\"\n",
        "        save_path += \"/rent.json\"\n",
        "    if i==2:\n",
        "        path += \"/중대재해처벌법(40,056건)\"\n",
        "        save_path += \"/accident.json\"\n",
        "    if i==3:\n",
        "        path += \"/차별금지법(59,421건)\" \n",
        "        save_path += \"/discrimination.json\"\n",
        "    if i==4:\n",
        "        path += \"/탄소중립(59,295건)\" \n",
        "        save_path += \"/neutral.json\"\n",
        "\n",
        "    print(path)\n",
        "    print(save_path)\n",
        "\n",
        "    data = load_all_data(path)\n",
        "    data.columns = data.iloc[0]\n",
        "    data.drop(0, inplace=True, axis=0)\n",
        "    data = data.dropna(axis=0, how='any')\n",
        "\n",
        "    data['내용'] = data['내용'].map(text_cleaning)\n",
        "\n",
        "    okt = Okt()\n",
        "    model = SentenceTransformer('jhgan/ko-sbert-multitask')\n",
        "\n",
        "    keywords = []\n",
        "    np.random.seed(42)\n",
        "    for i in tqdm(np.random.choice(data.index, 10000)):\n",
        "        tokenized_doc = okt.pos(data.iloc[i]['내용'], norm=True, stem=True)\n",
        "        tokenized_nouns = ' '.join([word[0] for word in tokenized_doc if word[1] == 'Noun'])\n",
        "\n",
        "        n_gram_range = (1,2)\n",
        "\n",
        "        count = CountVectorizer(ngram_range=n_gram_range).fit([tokenized_nouns])\n",
        "        candidates = count.get_feature_names_out()\n",
        "\n",
        "        candidate_embeddings = model.encode(candidates, convert_to_tensor=True)\n",
        "        doc_embedding = model.encode([tokenized_nouns], convert_to_tensor=True)\n",
        "        \n",
        "        keywords.append({data.iloc[i]['작성일']:max_sum_sim(doc_embedding.cpu(), candidate_embeddings.cpu(), candidates, top_n=5, nr_candidates=10)})\n",
        "\n",
        "    save_path = \"\"\n",
        "    if i==1:\n",
        "        save_path = \"/content/drive/MyDrive/aida/rent.json\"\n",
        "    if i==2:\n",
        "        save_path = \"/content/drive/MyDrive/aida/accident.json\"\n",
        "    if i==3:\n",
        "        save_path = \"/content/drive/MyDrive/aida/discrimination.json\" \n",
        "    if i==4:\n",
        "        save_path = \"/content/drive/MyDrive/aida/neutral.json\" \n",
        "      \n",
        "    with open(save_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(keywords, f, ensure_ascii=False, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "an0orQupbh5d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.7.13 ('ML')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "19d29624fa02f72a2f2eb64b5fa4dfbc751609e2b6c88be691c0db207c64cc14"
      }
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}